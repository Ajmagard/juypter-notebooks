{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import numpy as np\n",
    "import os\n",
    "from simple import *\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '../data/NER-RNN/ner'\n",
    "train_path = os.path.join(path, 'eng.train')\n",
    "testa_path = os.path.join(path, 'eng.testa')\n",
    "testb_path = os.path.join(path, 'eng.testb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_corpus(path):\n",
    "    groups = [[]]\n",
    "    for line in open(path):\n",
    "        parts = line.strip().split()\n",
    "        if len(parts):\n",
    "            groups[-1].append((parts[0], parts[-1]))\n",
    "        else:\n",
    "            groups.append([])\n",
    "    return groups\n",
    "\n",
    "all_groups = flatten([read_corpus(f) for f in [train_path, testa_path, testb_path]])[1:]\n",
    "all_tokens = flatten(all_groups)\n",
    "\n",
    "entities = list(set([tk[1] for tk in all_tokens]))\n",
    "chars = list(set(flatten([tk[0] for tk in all_tokens]))) + [' ']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Columns 0 to 12 \n",
      "   42    34    43    33    44    50    49    45    34    86    16    24    24\n",
      "   49    40    32    86    52    59    59    72    81    67    70    69    86\n",
      "   30    59    71    67    78    86    48    73    68    59    67    72    86\n",
      "   50    72    83    77    77    63    77    86    36    78    59    74    80\n",
      "   49    73    77    63    74    62    73    78    65    86    22    86    43\n",
      "   36    72    59    71    73    78    65    59    74    86    16    18    86\n",
      "   47    59    78    80    67    61    69    86    16    23    86    23    86\n",
      "   23    12    17    12    19    23    12    17    86    13    86    33    63\n",
      "   36    67    72    72    67    74    65    68    59    71    86    19    19\n",
      "   40    30    51    39    42    30    45    35    50    86    16    24    24\n",
      "   35    34    51    49    44    38    51    86    21    20    86    25    19\n",
      "   38    80    86    81    59    77    86    59    86    76    63    78    66\n",
      "\n",
      "Columns 13 to 25 \n",
      "   23    12    16    19    12    17    23    86     0     0     0     0     0\n",
      "   19    86    17    86    16    86    16    86    21    86    20    86    16\n",
      "   74    73    80    86    73    79    80    86    19    24    86     0     0\n",
      "   86     6    77    86    76    59    76    63    78    77    86    15    86\n",
      "   67    72    72    63    77    80    78    73    71    86    19    86     0\n",
      "   21    86    20    86    21    86    18    23    86    18    19    86    16\n",
      "   23    86    21    86    19    18    86    16    23    86    19    21    86\n",
      "   82    59    74    86    21    12    17    12    16    25    12    17    86\n",
      "   86    22    86    21    86    16    16    86    19    16    86    19    22\n",
      "   23    12    17    25    12    19    18    86     0     0     0     0     0\n",
      "   86    15    18    20    21    86    19    22    86    16    14    19    86\n",
      "   63    61    80    86    77    80    59    78    80    86    15    86     0\n",
      "\n",
      "Columns 26 to 29 \n",
      "    0     0     0     0\n",
      "   86     0     0     0\n",
      "    0     0     0     0\n",
      "    0     0     0     0\n",
      "    0     0     0     0\n",
      "   21    21    86     0\n",
      "    0     0     0     0\n",
      "   15    86     0     0\n",
      "   86    19    20    86\n",
      "    0     0     0     0\n",
      "    0     0     0     0\n",
      "    0     0     0     0\n",
      "[torch.LongTensor of size 12x30]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "char_lookup = {c: i+1 for i, c in enumerate(chars)}\n",
    "entity_lookup = {e: i for i, e in enumerate(entities)}\n",
    "\n",
    "def vectorize_sentence(tokens, bucket_rounding):\n",
    "    seq_len = sum([len(tk) for tk, ent in tokens]) + len(tokens)\n",
    "    seq_len = int(math.ceil(seq_len * 1.0 / bucket_rounding) * bucket_rounding)\n",
    "    x = torch.LongTensor(seq_len)\n",
    "    x.fill_(0)\n",
    "    y = torch.LongTensor(seq_len)\n",
    "    y.fill_(0)\n",
    "    i = 0\n",
    "    for token, ent in tokens:\n",
    "        ent_idx = entity_lookup[ent]\n",
    "        for c in token + ' ':\n",
    "            x[i] = char_lookup[c]\n",
    "            y[i] = ent_idx\n",
    "            i += 1\n",
    "    return x, y\n",
    "\n",
    "def epoch(batch_size=16):\n",
    "    sentences = all_groups[:]\n",
    "    random.shuffle(sentences)\n",
    "    buckets = defaultdict(list)\n",
    "    for sentence in sentences:\n",
    "        x, y = vectorize_sentence(sentence, 10)\n",
    "        bucket = x.size()[0]\n",
    "        buckets[bucket].append((x,y))\n",
    "        if len(buckets[bucket]) == batch_size:\n",
    "            xs = torch.stack([x for x,y in buckets[bucket]])\n",
    "            ys = torch.stack([y for x,y in buckets[bucket]])\n",
    "            yield xs, ys\n",
    "\n",
    "# print all_groups[0]\n",
    "# print vectorize_sentence(all_groups[0], 10)\n",
    "for x, y in epoch(12):\n",
    "    print x\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "input and gradOutput shapes do not match: input [16 x 20 x 8], gradOutput [16 x 8 x 20] at /home/soumith/local/builder/wheel/pytorch-src/torch/lib/THNN/generic/LogSoftMax.c:78",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-cadfc4dfb72b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mys\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m10\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/autograd/variable.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_variables)\u001b[0m\n\u001b[1;32m    143\u001b[0m                     'or with gradient w.r.t. the variable')\n\u001b[1;32m    144\u001b[0m             \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnew\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresize_as_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfill_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_execution_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_backward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_variables\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/torch/nn/_functions/thnn/auto.pyc\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, grad_output)\u001b[0m\n\u001b[1;32m    169\u001b[0m             \u001b[0mupdate_grad_input_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mupdate_grad_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m             \u001b[0mgi_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparams_without_bias\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0madditional_args\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 171\u001b[0;31m             \u001b[0mupdate_grad_input_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlibrary_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgi_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    172\u001b[0m             \u001b[0mgrad_input_tuple\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mgrad_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: input and gradOutput shapes do not match: input [16 x 20 x 8], gradOutput [16 x 8 x 20] at /home/soumith/local/builder/wheel/pytorch-src/torch/lib/THNN/generic/LogSoftMax.c:78"
     ]
    }
   ],
   "source": [
    "# the model has a field-of-view of 2**n_layers + 1 chars\n",
    "\n",
    "class NER(nn.Module):\n",
    "    def __init__(self, n_chars=len(chars)+1, n_entities=len(entities), embedding_dim=64, conv_layers=5):\n",
    "        super(NER, self).__init__()\n",
    "        self.dropout = torch.nn.Dropout(0.9)\n",
    "        self.convs = []\n",
    "        self.embedding = torch.nn.Embedding(n_chars, embedding_dim)\n",
    "        for i in xrange(conv_layers):\n",
    "            ksize = 3\n",
    "            dilation = 2 ** i\n",
    "            padding = dilation * (ksize-1) / 2\n",
    "            in_channels = embedding_dim if i == 0 else 256\n",
    "            c = torch.nn.Conv1d(in_channels, 256, ksize, dilation=dilation, padding=padding)\n",
    "            self.convs.append(c)\n",
    "        self.final_conv = torch.nn.Conv1d(256, n_entities, 1)\n",
    "        self.log_softmax = torch.nn.LogSoftmax()\n",
    "        self.criterion = torch.nn.NLLLoss2d()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x.data.transpose_(1, 2)\n",
    "        for c in self.convs:\n",
    "            x = self.dropout(F.relu(c(x)))\n",
    "        x = self.final_conv(x)\n",
    "        x.data.transpose_(1, 2) # [batch, seq_len, n_classes]\n",
    "        x = self.log_softmax(x)\n",
    "        \n",
    "        x.data.transpose_(1, 2) # [batch, n_classes, seq_len]\n",
    "        depth = x.size()[1]\n",
    "        seq_len = x.size()[2]\n",
    "        x = x.contiguous().view(-1, depth, 1, seq_len) # [batch, n_classes, 1, seq_len]\n",
    "        return x\n",
    "    \n",
    "    def infer(self, x):\n",
    "        x_batch = torch.stack([x])\n",
    "        result = self.forward(x_batch).data.numpy()[0]\n",
    "        ents = [entities[idx] for idx in np.argmax(result, axis=1)]\n",
    "        return ents\n",
    "    \n",
    "    def compute_loss(self, xs, ys):\n",
    "        x = self.forward(xs)\n",
    "        seq_len = ys.size()[1]\n",
    "        y = ys.contiguous().view(-1, 1, seq_len)\n",
    "        return self.criterion(x, y)\n",
    "\n",
    "# sent = all_groups[0]\n",
    "# x, y = vectorize_sentence(sent, 10)\n",
    "# print NER().infer(Variable(x))\n",
    "\n",
    "net = NER()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(net.parameters(), lr=0.0001)\n",
    "\n",
    "for i, (xs, ys) in enumerate(epoch()):\n",
    "    xs = Variable(xs)\n",
    "    ys = Variable(ys)\n",
    "    optimizer.zero_grad()\n",
    "    loss = net.compute_loss(xs, ys)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if i % 10 == 0:\n",
    "        print 'Loss:', loss\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
