{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os\n",
    "from simple import *\n",
    "import math\n",
    "from collections import defaultdict\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "path = '../data/NER-RNN/ner'\n",
    "train_path = os.path.join(path, 'eng.train')\n",
    "testa_path = os.path.join(path, 'eng.testa')\n",
    "testb_path = os.path.join(path, 'eng.testb')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def read_corpus(path):\n",
    "    sents = [[]]\n",
    "    for line in open(path):\n",
    "        parts = line.strip().split()\n",
    "        if len(parts):\n",
    "            sents[-1].append((parts[0], parts[-1]))\n",
    "        else:\n",
    "            sents.append([])\n",
    "    return sents\n",
    "\n",
    "all_sents = flatten([read_corpus(f) for f in [train_path, testa_path, testb_path]])[1:]\n",
    "all_tokens = flatten(all_sents)\n",
    "\n",
    "entities = list(set([tk[1] for tk in all_tokens]))\n",
    "chars = list(set(flatten([tk[0] for tk in all_tokens]))) + [' ']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "char_lookup = {c: i+1 for i, c in enumerate(chars)}\n",
    "entity_lookup = {e: i for i, e in enumerate(entities)}\n",
    "\n",
    "PADDING = 32 # should correspond roughly to field of view of the model\n",
    "\n",
    "total_len = PADDING\n",
    "for sent in all_sents:\n",
    "    total_len += sum([len(tk) for tk, ent in sent]) + max(len(sent) - 1, 0) + PADDING\n",
    "\n",
    "x = np.zeros(total_len, dtype=int)\n",
    "y = np.zeros(total_len, dtype=int)\n",
    "\n",
    "i = PADDING\n",
    "for sent in all_sents:\n",
    "    for j, (token, entity) in enumerate(sent):\n",
    "        x[i:i+len(token)] = [char_lookup[c] for c in token]\n",
    "        y[i:i+len(token)] = [entity_lookup[entity]] * len(token)\n",
    "        i += len(token)\n",
    "        if j+1 < len(sent):\n",
    "            x[i] = char_lookup[' ']\n",
    "            y[i] += entity_lookup[entity]\n",
    "            i += 1\n",
    "    i += PADDING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "call to boycott British lamb .\n"
     ]
    }
   ],
   "source": [
    "def tensor_to_text(t):\n",
    "    return u''.join([chars[i-1] for i in t if i > 0])\n",
    "\n",
    "print tensor_to_text(x[50:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Net(object):\n",
    "    def __init__(self):\n",
    "        self.global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "        \n",
    "    session = None\n",
    "    \n",
    "    def setup(self, path=None):\n",
    "        if self.session: self.session.close()\n",
    "        self.session = tf.InteractiveSession()\n",
    "        \n",
    "        init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "        self.session.run(init_op)\n",
    "        tf.train.start_queue_runners(sess=self.session)\n",
    "        \n",
    "        self.saver = None\n",
    "        self.path = path\n",
    "        if path:\n",
    "            if not os.path.exists(path):\n",
    "                os.mkdir(path)\n",
    "            self.saver = tf.train.Saver()\n",
    "            ckpt = tf.train.get_checkpoint_state(path)\n",
    "            if ckpt and ckpt.model_checkpoint_path:\n",
    "                saver.restore(self.session, ckpt.model_checkpoint_path)\n",
    "                print 'Restored from checkpoint', ckpt.model_checkpoint_path\n",
    "            else:\n",
    "                print 'Did not restore from checkpoint'\n",
    "        else:\n",
    "            print 'Will not save progress'\n",
    "    \n",
    "    def save(self):\n",
    "        if self.saver:\n",
    "            step_ = self.session.run(self.global_step)\n",
    "            saver.save(session, self.path + '/model.ckpt', global_step=step_)\n",
    "            print 'Saved'\n",
    "    \n",
    "    def train_step(self, verbose):\n",
    "        # should return the current step\n",
    "        assert False, 'train_step() not implemented'\n",
    "    \n",
    "    def train(self, print_every=2, save_every=100):\n",
    "        while True:\n",
    "            step_ = self.session.run(self.global_step)\n",
    "            verbose = (step_ % print_every == 0)\n",
    "            self.train_step(verbose)\n",
    "            if step_ % save_every == 1:\n",
    "                self.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "field of view: 17\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "def identity(x):\n",
    "    return x\n",
    "\n",
    "class NER(Net):\n",
    "    def __init__(self, x, y):\n",
    "        with tf.variable_scope('ner8'):\n",
    "            super(NER, self).__init__()\n",
    "            \n",
    "            batch_size = 32\n",
    "            length = 64\n",
    "            \n",
    "            indices = [tf.random_uniform([], minval=0, maxval=len(x) - length, dtype=tf.int64) for _ in xrange(batch_size)]\n",
    "            x_batch = tf.stack([tf.slice(x, [idx], [length]) for idx in indices])\n",
    "            y_batch = tf.stack([tf.slice(y, [idx], [length]) for idx in indices])\n",
    "            \n",
    "            inferred = self.fwd(x_batch)\n",
    "\n",
    "            loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y_batch, logits=inferred))\n",
    "\n",
    "            self.train_op = tf.train.AdamOptimizer(1e-3).minimize(loss, global_step=self.global_step)\n",
    "\n",
    "            self.loss = loss\n",
    "            self.x_batch = x_batch\n",
    "            self.y_batch = y_batch\n",
    "            self.inferred = inferred\n",
    "\n",
    "            self.losses = []\n",
    "    \n",
    "    def fwd(self, batch):\n",
    "        with tf.variable_scope('fwd'):\n",
    "            depth = len(chars) + 1\n",
    "            length = batch.get_shape()[-1]\n",
    "            batch = tf.one_hot(batch, depth)\n",
    "\n",
    "            n_layers = 4\n",
    "            fov = 1 + 2 ** (n_layers)\n",
    "            print 'field of view:', fov\n",
    "            for i in xrange(n_layers):\n",
    "                channels = 32 * 2**i\n",
    "                convolved = tf.layers.conv1d(batch, channels, 3, dilation_rate=2**i, padding='same', activation=tf.nn.relu)\n",
    "                batch = tf.concat([batch, convolved], 2) # skip connection\n",
    "                batch = tf.layers.batch_normalization(batch)\n",
    "            batch = tf.layers.conv1d(batch, len(entities), 1, activation=identity)\n",
    "            return batch\n",
    "    \n",
    "    def train_step(self, verbose):\n",
    "        loss_, step_, _ = self.session.run([self.loss, self.global_step, self.train_op])\n",
    "        self.losses.append(loss_)\n",
    "        if verbose:\n",
    "            print \"{}: {}\".format(step_, sum(self.losses) / len(self.losses))\n",
    "            self.losses = []\n",
    "\n",
    "n = NER(x, y)\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
