{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib import layers\n",
    "import numpy as np\n",
    "import math\n",
    "import os\n",
    "from nbutil import imshow_multi\n",
    "%matplotlib inline\n",
    "from simple import flatten, avg\n",
    "import random\n",
    "from show_graph import show_graph\n",
    "from skimage import io\n",
    "from skimage.transform import resize\n",
    "import charttt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 32\n",
    "N_CLASSES = 102 # 101 foods + null class\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 32, 32, 16)\n",
      "(?, 16, 16, 32)\n",
      "(?, 8, 8, 64)\n",
      "(?, 4, 4, 128)\n",
      "(?, 2, 2, 256)\n",
      "(?, 1, 1, 512)\n",
      "ok\n"
     ]
    }
   ],
   "source": [
    "# bias_init = tf.truncated_normal_initializer(mean=0.1, stddev=0.02)\n",
    "\n",
    "class Net(object):\n",
    "    def __init__(self, img_size, n_classes):\n",
    "        with tf.variable_scope('net'):\n",
    "            n_layers = int(math.log(img_size) / math.log(2))\n",
    "\n",
    "            inputs = tf.placeholder(tf.float32, [None, img_size, img_size, 3], name='inputs')\n",
    "            labels = tf.placeholder(tf.int64, [None], name='labels')\n",
    "            dropout = tf.placeholder(tf.float32, name='dropout')\n",
    "\n",
    "            initial_chans = 16\n",
    "            x = self.conv(inputs, initial_chans, 1)\n",
    "            print x.get_shape()\n",
    "\n",
    "            for i, layer in enumerate(xrange(n_layers)):\n",
    "                chans = initial_chans * 2 ** (i+1)\n",
    "                x = self.inception_module(x, chans, 'inception-{}'.format(i))\n",
    "                x = layers.batch_norm(x)\n",
    "                x = self.inception_module(x, chans, 'inception-{}-2'.format(i))\n",
    "                x = layers.batch_norm(x)\n",
    "                if i % 2 == 0:\n",
    "                    x = tf.nn.dropout(x, dropout)\n",
    "                x = self.pool(x, 2, 2, type='avg')\n",
    "                print x.get_shape()\n",
    "            \n",
    "            logits = self.conv(x, n_classes, 1, activation=tf.identity)\n",
    "            logits = tf.reshape(logits, [-1, n_classes])\n",
    "\n",
    "            loss = tf.reduce_sum(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=labels, logits=logits))\n",
    "            predictions = tf.argmax(logits, axis=-1)\n",
    "            accuracy = tf.reduce_mean(tf.cast(tf.equal(predictions, labels), tf.float32))\n",
    "\n",
    "            global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "            lr = tf.placeholder(tf.float32, name='lr')\n",
    "            train_op = tf.train.AdamOptimizer(lr).minimize(loss, global_step=global_step)\n",
    "\n",
    "            self.inputs = inputs\n",
    "            self.labels = labels\n",
    "            self.loss = loss\n",
    "            self.logits = logits\n",
    "            self.predictions = predictions\n",
    "            self.accuracy = accuracy\n",
    "            self.global_step = global_step\n",
    "            self.train_op = train_op\n",
    "            self.lr = lr\n",
    "            self.dropout = dropout\n",
    "    \n",
    "    def conv(self, x, channels, ksize, activation=tf.nn.relu):\n",
    "        return layers.conv2d(x, \n",
    "                             channels,\n",
    "                             kernel_size=ksize, \n",
    "                             activation_fn=activation)\n",
    "    \n",
    "    def pool(self, x, ksize, stride, type='max'):\n",
    "        fn = {'max': tf.nn.max_pool, 'avg': tf.nn.avg_pool}[type]\n",
    "        return fn(x, [1, ksize, ksize, 1], [1, stride, stride, 1], 'SAME')\n",
    "    \n",
    "    def inception_module(self, x, out_channels, name):\n",
    "        # https://culurciello.github.io/tech/2016/06/04/nets.html\n",
    "        # out_channels channels must be a multiple of 4\n",
    "        \n",
    "        with tf.variable_scope(name):\n",
    "            channels = x.get_shape()[-1].value\n",
    "            bsize = out_channels / 4\n",
    "            \n",
    "            y1 = self.conv(x, bsize, 1)\n",
    "            y2 = self.conv(self.pool(x, 3, 1, 'avg'), bsize, 1)\n",
    "            y3 = self.conv(self.conv(x, bsize, 1), bsize, 3)\n",
    "            y4 = self.conv(self.conv(self.conv(x, bsize, 1), bsize, 3), bsize, 3)\n",
    "            \n",
    "            return tf.concat([y1, y2, y3, y4], axis=-1)\n",
    "\n",
    "n = Net(IMAGE_SIZE, N_CLASSES)\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inception module v3 \n",
    "\n",
    "<img src='https://culurciello.github.io/assets/nets/inceptionv3.jpg' width=300 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple_pie', 'baby_back_ribs', 'baklava', 'beef_carpaccio', 'beef_tartare', 'beet_salad', 'beignets', 'bibimbap', 'bread_pudding', 'breakfast_burrito', 'bruschetta', 'caesar_salad', 'cannoli', 'caprese_salad', 'carrot_cake', 'ceviche', 'cheese_plate', 'cheesecake', 'chicken_curry', 'chicken_quesadilla', 'chicken_wings', 'chocolate_cake', 'chocolate_mousse', 'churros', 'clam_chowder', 'club_sandwich', 'crab_cakes', 'creme_brulee', 'croque_madame', 'cup_cakes', 'deviled_eggs', 'donuts', 'dumplings', 'edamame', 'eggs_benedict', 'escargots', 'falafel', 'filet_mignon', 'fish_and_chips', 'foie_gras', 'french_fries', 'french_onion_soup', 'french_toast', 'fried_calamari', 'fried_rice', 'frozen_yogurt', 'garlic_bread', 'gnocchi', 'greek_salad', 'grilled_cheese_sandwich', 'grilled_salmon', 'guacamole', 'gyoza', 'hamburger', 'hot_and_sour_soup', 'hot_dog', 'huevos_rancheros', 'hummus', 'ice_cream', 'lasagna', 'lobster_bisque', 'lobster_roll_sandwich', 'macaroni_and_cheese', 'macarons', 'miso_soup', 'mussels', 'nachos', 'null', 'omelette', 'onion_rings', 'oysters', 'pad_thai', 'paella', 'pancakes', 'panna_cotta', 'peking_duck', 'pho', 'pizza', 'pork_chop', 'poutine', 'prime_rib', 'pulled_pork_sandwich', 'ramen', 'ravioli', 'red_velvet_cake', 'risotto', 'samosa', 'sashimi', 'scallops', 'seaweed_salad', 'shrimp_and_grits', 'spaghetti_bolognese', 'spaghetti_carbonara', 'spring_rolls', 'steak', 'strawberry_shortcake', 'sushi', 'tacos', 'takoyaki', 'tiramisu', 'tuna_tartare', 'waffles']\n"
     ]
    }
   ],
   "source": [
    "root = '../data/food-101/images'\n",
    "labels = sorted(os.listdir(root))\n",
    "\n",
    "paths_to_labels = {}\n",
    "\n",
    "for label in labels:\n",
    "    label_dir = os.path.join(root, label)\n",
    "    for name in os.listdir(label_dir):\n",
    "        if name.endswith('.jpg') or name.endswith('.png') or name.endswith('.gif'):\n",
    "            path = os.path.join(label_dir, name)\n",
    "            paths_to_labels[path] = label\n",
    "\n",
    "print labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def create_image_queue(filenames, batch_size=BATCH_SIZE, grayscale=False, size=128):\n",
    "    filename_tensor = tf.convert_to_tensor(filenames, dtype=tf.string)\n",
    "    filename_q = tf.train.slice_input_producer([filename_tensor], num_epochs=None, shuffle=True)[0]\n",
    "\n",
    "    image_255 = tf.image.decode_jpeg(tf.read_file(filename_q), channels=3)\n",
    "    image = tf.cast(image_255, tf.float32) / 255.0\n",
    "    if grayscale:\n",
    "        image = tf.image.grayscale_to_rgb(image)\n",
    "    \n",
    "    def resize_image(image):\n",
    "        pre_crop_size = tf.cast(size + tf.random_uniform([]) * size * 0.3, tf.int32)\n",
    "        image_as_batch_of_1 = tf.expand_dims(image, 0)\n",
    "        image = tf.image.resize_bilinear(image_as_batch_of_1, [pre_crop_size, pre_crop_size])[0]\n",
    "        image = tf.random_crop(image, [size, size, 3])\n",
    "        return image\n",
    "    \n",
    "    image = resize_image(image)\n",
    "\n",
    "    def distort_image(image):\n",
    "        # noise_amt = tf.abs(tf.random_normal([], stddev=0.2))\n",
    "        # distorted_image = image + tf.random_uniform([299, 299, 3], maxval=noise_amt)\n",
    "        distorted_image = tf.image.random_flip_left_right(image)\n",
    "        distorted_image = tf.image.random_brightness(distorted_image, max_delta=0.15)\n",
    "        distorted_image = tf.image.random_contrast(distorted_image, lower=0.8, upper=1.2)\n",
    "        return distorted_image\n",
    "    \n",
    "    image = distort_image(image)\n",
    "    \n",
    "    images_batch, filenames_batch = tf.train.shuffle_batch([image, filename_q], batch_size=batch_size, capacity=128, min_after_dequeue=64)\n",
    "    return images_batch, filenames_batch\n",
    "\n",
    "images_batch, filenames_batch = create_image_queue(paths_to_labels.keys(), size=IMAGE_SIZE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# sess = tf.Session()\n",
    "# images_batch, filenames_batch = create_image_queue(paths_to_labels.keys(), size=IMAGE_SIZE)\n",
    "\n",
    "# init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "# sess.run(init_op)\n",
    "# tf.train.start_queue_runners(sess=sess)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# show_graph(sess.graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# images, filenames = sess.run([images_batch, filenames_batch])\n",
    "# imshow_multi(list(images)[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:net/global_step/sec: 0\n",
      "1436: loss=246.971046448, acc=0.12890625, epoch=0.909940594059\n",
      "1440: loss=259.672866821, acc=0.10546875, epoch=0.912475247525\n",
      "1444: loss=245.661331177, acc=0.125, epoch=0.91500990099\n",
      "1448: loss=243.870147705, acc=0.10546875, epoch=0.917544554455\n",
      "1452: loss=252.35993576, acc=0.13671875, epoch=0.920079207921\n",
      "1456: loss=232.575698853, acc=0.1484375, epoch=0.922613861386\n",
      "1460: loss=243.758975983, acc=0.109375, epoch=0.925148514851\n",
      "1464: loss=239.572036743, acc=0.12890625, epoch=0.927683168317\n",
      "1468: loss=248.127399445, acc=0.12109375, epoch=0.930217821782\n",
      "1472: loss=243.621707916, acc=0.1015625, epoch=0.932752475248\n",
      "1476: loss=251.436630249, acc=0.12109375, epoch=0.935287128713\n",
      "1480: loss=243.540611267, acc=0.1015625, epoch=0.937821782178\n",
      "1484: loss=243.528594971, acc=0.12109375, epoch=0.940356435644\n",
      "1488: loss=241.463428497, acc=0.12109375, epoch=0.942891089109\n",
      "1492: loss=229.575645447, acc=0.13671875, epoch=0.945425742574\n",
      "1496: loss=232.070968628, acc=0.140625, epoch=0.94796039604\n",
      "1500: loss=241.189220428, acc=0.12109375, epoch=0.950495049505\n",
      "1504: loss=244.096305847, acc=0.1328125, epoch=0.95302970297\n",
      "1508: loss=243.195362091, acc=0.12109375, epoch=0.955564356436\n",
      "1512: loss=242.901180267, acc=0.11328125, epoch=0.958099009901\n",
      "1516: loss=248.791793823, acc=0.09765625, epoch=0.960633663366\n",
      "1520: loss=253.805271149, acc=0.09375, epoch=0.963168316832\n",
      "1524: loss=240.970493317, acc=0.140625, epoch=0.965702970297\n",
      "1528: loss=255.822601318, acc=0.08203125, epoch=0.968237623762\n",
      "1532: loss=242.912666321, acc=0.1171875, epoch=0.970772277228\n",
      "1536: loss=238.313461304, acc=0.14453125, epoch=0.973306930693\n",
      "1540: loss=234.879634857, acc=0.13671875, epoch=0.975841584158\n",
      "1544: loss=240.448879242, acc=0.12109375, epoch=0.978376237624\n",
      "1548: loss=247.481891632, acc=0.125, epoch=0.980910891089\n",
      "1552: loss=243.458580017, acc=0.09765625, epoch=0.983445544554\n",
      "1556: loss=246.273529053, acc=0.1171875, epoch=0.98598019802\n",
      "1560: loss=235.211055756, acc=0.1484375, epoch=0.988514851485\n",
      "1564: loss=236.968898773, acc=0.14453125, epoch=0.99104950495\n",
      "1568: loss=247.650276184, acc=0.125, epoch=0.993584158416\n",
      "1572: loss=236.160411835, acc=0.1484375, epoch=0.996118811881\n",
      "1576: loss=234.926544189, acc=0.1328125, epoch=0.998653465347\n",
      "1580: loss=242.838531494, acc=0.15234375, epoch=1.00118811881\n",
      "1584: loss=239.545463562, acc=0.125, epoch=1.00372277228\n",
      "1588: loss=242.377513885, acc=0.12109375, epoch=1.00625742574\n",
      "1592: loss=246.784576416, acc=0.140625, epoch=1.00879207921\n",
      "1596: loss=229.7044487, acc=0.1796875, epoch=1.01132673267\n",
      "1600: loss=236.768428802, acc=0.1484375, epoch=1.01386138614\n",
      "1604: loss=239.217636108, acc=0.1328125, epoch=1.0163960396\n",
      "1608: loss=253.95267868, acc=0.08984375, epoch=1.01893069307\n",
      "1612: loss=236.511455536, acc=0.1484375, epoch=1.02146534653\n",
      "1616: loss=245.403141022, acc=0.14453125, epoch=1.024\n",
      "1620: loss=242.428844452, acc=0.13671875, epoch=1.02653465347\n",
      "1624: loss=240.237422943, acc=0.140625, epoch=1.02906930693\n",
      "1628: loss=245.844169617, acc=0.1328125, epoch=1.0316039604\n",
      "1632: loss=247.148750305, acc=0.12109375, epoch=1.03413861386\n",
      "1636: loss=246.562503815, acc=0.1171875, epoch=1.03667326733\n",
      "1640: loss=234.849807739, acc=0.16015625, epoch=1.03920792079\n",
      "1644: loss=245.28786087, acc=0.109375, epoch=1.04174257426\n",
      "1648: loss=241.17949295, acc=0.1328125, epoch=1.04427722772\n",
      "1652: loss=240.858879089, acc=0.1328125, epoch=1.04681188119\n",
      "1656: loss=242.670509338, acc=0.11328125, epoch=1.04934653465\n",
      "1660: loss=238.948722839, acc=0.11328125, epoch=1.05188118812\n",
      "1664: loss=242.877925873, acc=0.1171875, epoch=1.05441584158\n",
      "1668: loss=240.469623566, acc=0.1171875, epoch=1.05695049505\n",
      "1672: loss=243.46893692, acc=0.1171875, epoch=1.05948514851\n",
      "1676: loss=233.828922272, acc=0.16796875, epoch=1.06201980198\n",
      "1680: loss=243.575061798, acc=0.12890625, epoch=1.06455445545\n",
      "1684: loss=242.143604279, acc=0.1328125, epoch=1.06708910891\n",
      "1688: loss=234.100715637, acc=0.16796875, epoch=1.06962376238\n",
      "1692: loss=236.225788116, acc=0.14453125, epoch=1.07215841584\n",
      "1696: loss=246.22019577, acc=0.12890625, epoch=1.07469306931\n",
      "1700: loss=235.475036621, acc=0.14453125, epoch=1.07722772277\n",
      "1704: loss=236.312324524, acc=0.12109375, epoch=1.07976237624\n",
      "1708: loss=238.526237488, acc=0.125, epoch=1.0822970297\n",
      "1712: loss=234.361839294, acc=0.15625, epoch=1.08483168317\n",
      "1716: loss=236.355606079, acc=0.1328125, epoch=1.08736633663\n",
      "1720: loss=253.922523499, acc=0.09375, epoch=1.0899009901\n",
      "1724: loss=255.173397064, acc=0.10546875, epoch=1.09243564356\n",
      "1728: loss=242.25415802, acc=0.15625, epoch=1.09497029703\n",
      "1732: loss=250.488182068, acc=0.10546875, epoch=1.0975049505\n",
      "1736: loss=234.265552521, acc=0.140625, epoch=1.10003960396\n",
      "1740: loss=243.306529999, acc=0.1484375, epoch=1.10257425743\n",
      "1744: loss=230.097072601, acc=0.12109375, epoch=1.10510891089\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-a3e7addd2f06>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mstep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglobal_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimages_batch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilenames_batch\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mlabel_strings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpaths_to_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mpath\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mfilenames\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mlabel_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlabel_strings\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "chart = charttt.Board('foodnetwork').chart('foodnetwork 32 4')\n",
    "\n",
    "sv = tf.train.Supervisor(logdir='models/foodnetwork-32-4', save_model_secs=60 * 5)\n",
    "with sv.managed_session() as sess:\n",
    "    \n",
    "    # init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    # sess.run(init_op)\n",
    "    tf.train.start_queue_runners(sess=sess)\n",
    "    \n",
    "    losses = []\n",
    "    accs = []\n",
    "    \n",
    "    while not sv.should_stop():\n",
    "        step = sess.run(n.global_step)\n",
    "        images, filenames = sess.run([images_batch, filenames_batch])\n",
    "        label_strings = [paths_to_labels[path] for path in filenames]\n",
    "        label_indices = [labels.index(label) for label in label_strings]\n",
    "        \n",
    "        epoch = step * BATCH_SIZE * 1.0 / len(paths_to_labels)\n",
    "        lr = 0.0005\n",
    "        # if epoch > 1: lr *= 0.5\n",
    "        # if epoch > 10: lr *= 0.5\n",
    "        \n",
    "        _, loss, acc = sess.run([n.train_op, n.loss, n.accuracy], feed_dict={\n",
    "            n.lr: lr,\n",
    "            n.inputs: images,\n",
    "            n.labels: label_indices,\n",
    "            n.dropout: 0.7\n",
    "        })\n",
    "        losses.append(loss)\n",
    "        accs.append(acc)\n",
    "        # print logits[0]\n",
    "        if step % 4 == 0:\n",
    "            print \"{}: loss={}, acc={}, epoch={}\".format(step, avg(losses), avg(accs), epoch)\n",
    "            chart.write(epoch, loss=avg(losses), accuracy=avg(accs))\n",
    "            losses = []\n",
    "            accs = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "# create a directory of 1000 random images:\n",
    "\n",
    "def create_null_dir():\n",
    "\n",
    "    import shutil\n",
    "\n",
    "    print nulldir\n",
    "\n",
    "    def iterate_images_recursively(path, extensions=['.jpeg', '.jpg', '.png', '.gif']):\n",
    "        def is_image(path):\n",
    "            for ext in extensions:\n",
    "                if path.lower().endswith(ext):\n",
    "                    return True\n",
    "            return False\n",
    "\n",
    "        for (dirpath, dirname, filenames) in os.walk(path):\n",
    "            for filename in filenames:\n",
    "                subpath = os.path.join(dirpath, filename)\n",
    "                if is_image(subpath):\n",
    "                    yield subpath\n",
    "\n",
    "    tiny_imagenet_images = list(iterate_images_recursively('../data/tiny-imagenet-200/train'))\n",
    "    floor_images = list(iterate_images_recursively('../data/floors'))\n",
    "\n",
    "    nulldir = os.path.join(root, 'null')\n",
    "    try:\n",
    "        os.rmdir(nulldir)\n",
    "    except OSError:\n",
    "        pass\n",
    "    os.mkdir(nulldir)\n",
    "\n",
    "    for i in xrange(1000):\n",
    "        path = random.choice(random.choice([tiny_imagenet_images, floor_images]))\n",
    "        ext = path.split('.')[-1]\n",
    "        name = os.path.join(nulldir, '{}.{}'.format(i, ext))\n",
    "        shutil.copyfile(path, name)\n",
    "\n",
    "# create_null_dir()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "! ls ../data/food-101/images/null"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    return np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "\n",
    "with sv.managed_session() as sess:\n",
    "\n",
    "    img = io.imread('http://3.bp.blogspot.com/-agOrGBzjpKw/UHMB77A5NnI/AAAAAAAAD0Q/wOL7KXEh1-Y/s1600/DSC_0422.JPG')\n",
    "    img = resize(img, [IMAGE_SIZE, IMAGE_SIZE])\n",
    "    img = np.expand_dims(img, 0)\n",
    "    print img.shape\n",
    "\n",
    "    feed = {\n",
    "        n.dropout: 1,\n",
    "        n.inputs: img\n",
    "    }\n",
    "    predictions = softmax(sess.run(n.logits, feed_dict=feed)[0])\n",
    "    indices = sorted(range(102), key=lambda i: predictions[i], reverse=True)\n",
    "    for i in indices:\n",
    "        print labels[i], ':', predictions[i]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
