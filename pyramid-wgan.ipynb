{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from nbutil import imshow_multi\n",
    "import os\n",
    "from tensorflow.contrib.layers.python.layers import batch_norm\n",
    "from tensorflow.contrib.layers import xavier_initializer\n",
    "from PIL import Image\n",
    "import urllib, cStringIO\n",
    "# import cv2\n",
    "from tensorflow.contrib import layers\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "PYRAMID_LAYERS = 2\n",
    "LAYER_UPSCALE = 4\n",
    "MAX_SIZE = (LAYER_UPSCALE ** PYRAMID_LAYERS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "attr_file = '../data/celeba/list_attr_celeba.txt'\n",
    "keys = None\n",
    "attributes_by_image = {}\n",
    "for i, line in enumerate(open(attr_file)):\n",
    "    if i == 1:\n",
    "        keys = line.split()\n",
    "    elif i > 1:\n",
    "        image = os.path.join('../data/celeba/img_align_celeba', line.split()[0])\n",
    "        values = line.split()[1:]\n",
    "        attributes_by_image[image] = {attr: val == '1' for attr, val in zip(keys, values)}\n",
    "\n",
    "image_names = attributes_by_image.keys()\n",
    "attr_vector = np.zeros((len(attributes_by_image), len(keys)))\n",
    "for i, image_name in enumerate(image_names):\n",
    "    attrs = attributes_by_image[image_name]\n",
    "    attr_vector[i] = [(1 if attrs[key] else 0) for key in keys]\n",
    "\n",
    "def create_qs(train):\n",
    "    filename_tensor = tf.convert_to_tensor(image_names, dtype=tf.string)\n",
    "    attr_tensor = tf.convert_to_tensor(attr_vector, dtype=tf.float32)\n",
    "    filename_q, attr_q = tf.train.slice_input_producer([filename_tensor, attr_tensor], num_epochs=None, shuffle=True)\n",
    "\n",
    "    # reader = tf.WholeFileReader()\n",
    "    # filename, image_data = reader.read(filename_q)\n",
    "    image_255 = tf.image.decode_jpeg(tf.read_file(filename_q))\n",
    "    image = tf.reshape(tf.cast(image_255, tf.float32) / 255.0, [218, 178, 3]) # images are 178x218\n",
    "    # image = tf.image.rgb_to_grayscale(image)\n",
    "\n",
    "    def resize_image(image):\n",
    "        # image = tf.random_crop(image, [192, 160, 3])\n",
    "        # return image\n",
    "        image = tf.image.resize_image_with_crop_or_pad(image, 160, 160)\n",
    "        img_reshaped = tf.reshape(image, [1, 160, 160, 3])\n",
    "        image = tf.image.resize_bilinear(img_reshaped, [MAX_SIZE, MAX_SIZE])\n",
    "        return tf.reshape(image, [MAX_SIZE, MAX_SIZE, 3])\n",
    "    image = resize_image(image)\n",
    "\n",
    "    def distort_image(image):\n",
    "        # noise_amt = tf.abs(tf.random_normal([], stddev=0.2))\n",
    "        # distorted_image = image + tf.random_uniform([64, 64, 3], maxval=noise_amt)\n",
    "        distorted_image = tf.image.random_flip_left_right(image)\n",
    "        distorted_image = tf.image.random_brightness(distorted_image, max_delta=0.3)\n",
    "        distorted_image = tf.image.random_contrast(distorted_image, lower=0.6, upper=1.6)\n",
    "        # distorted_image = tf.image.per_image_standardization(distorted_image)\n",
    "        # distorted_image = tf.clip_by_value(distorted_image, 0, 1)\n",
    "        return distorted_image\n",
    "\n",
    "    images_batch, attrs_batch = tf.train.shuffle_batch([distort_image(image), attr_q], batch_size=BATCH_SIZE, capacity=BATCH_SIZE*20, min_after_dequeue=BATCH_SIZE*10)\n",
    "    return images_batch, attrs_batch\n",
    "\n",
    "images_batch, attrs_batch = create_qs(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5_o_Clock_Shadow',\n",
       " 'Arched_Eyebrows',\n",
       " 'Attractive',\n",
       " 'Bags_Under_Eyes',\n",
       " 'Bald',\n",
       " 'Bangs',\n",
       " 'Big_Lips',\n",
       " 'Big_Nose',\n",
       " 'Black_Hair',\n",
       " 'Blond_Hair',\n",
       " 'Blurry',\n",
       " 'Brown_Hair',\n",
       " 'Bushy_Eyebrows',\n",
       " 'Chubby',\n",
       " 'Double_Chin',\n",
       " 'Eyeglasses',\n",
       " 'Goatee',\n",
       " 'Gray_Hair',\n",
       " 'Heavy_Makeup',\n",
       " 'High_Cheekbones',\n",
       " 'Male',\n",
       " 'Mouth_Slightly_Open',\n",
       " 'Mustache',\n",
       " 'Narrow_Eyes',\n",
       " 'No_Beard',\n",
       " 'Oval_Face',\n",
       " 'Pale_Skin',\n",
       " 'Pointy_Nose',\n",
       " 'Receding_Hairline',\n",
       " 'Rosy_Cheeks',\n",
       " 'Sideburns',\n",
       " 'Smiling',\n",
       " 'Straight_Hair',\n",
       " 'Wavy_Hair',\n",
       " 'Wearing_Earrings',\n",
       " 'Wearing_Hat',\n",
       " 'Wearing_Lipstick',\n",
       " 'Wearing_Necklace',\n",
       " 'Wearing_Necktie',\n",
       " 'Young']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ok\n"
     ]
    }
   ],
   "source": [
    "initializer = tf.truncated_normal_initializer(0.00, 0.02)\n",
    "\n",
    "def lrelu(x):\n",
    "    # leaky relu\n",
    "    alpha = 0.1\n",
    "    return tf.maximum(alpha*x,x)\n",
    "\n",
    "def identity(x): return x\n",
    "\n",
    "def l2_loss(y, y_):\n",
    "    return (y - y_) * (y - y_)\n",
    "\n",
    "Z_SIZE = 32\n",
    "\n",
    "def upscaler(img, z_vec, scope):\n",
    "    # img can be none:\n",
    "    with tf.variable_scope(scope):\n",
    "        orig = img\n",
    "        n_layers = int(round(math.log(LAYER_UPSCALE) / math.log(2)))\n",
    "        z_reshaped = tf.reshape(tf.cast(z_vec, tf.float32), [-1, 1, 1, z_vec.get_shape()[-1].value])\n",
    "        for i in xrange(n_layers):\n",
    "            n_channels = 32 ** (n_layers - 1 - i)\n",
    "            img = layers.conv2d(img,\n",
    "                      n_channels * 2, \n",
    "                      scope='up_'+str(i), \n",
    "                      kernel_size=4, \n",
    "                      activation_fn=lrelu, \n",
    "                      stride=2,\n",
    "                      normalizer_fn=layers.batch_norm, \n",
    "                      weights_initializer=initializer)\n",
    "            \n",
    "            _, h, w, _ = [x.value for x in img.get_shape()]\n",
    "            z_tiled = tf.tile(z_reshaped, [1, h, w, 1])\n",
    "            img = tf.concat([img, z_tiled], axis=3)\n",
    "            \n",
    "            if i + 1 < n_layers:\n",
    "                act = lrelu\n",
    "                out_channels = n_channels\n",
    "            else:\n",
    "                act = tf.tanh\n",
    "                out_channels = 3\n",
    "            img = layers.conv2d(img, \n",
    "                                n_channels, \n",
    "                                scope='up_1x1_'+str(i),\n",
    "                                kernel_size=1,\n",
    "                                stride=1,\n",
    "                                activation_fn=act,\n",
    "                                weights_initializer=initializer)\n",
    "        \n",
    "        _, h, w, _ = [x.value for x in orig.get_shape()]\n",
    "        small_again = tf.image.resize_images(img, [h, w])\n",
    "        sampling_loss = tf.reduce_mean(l2_loss(img, small_again))\n",
    "        return img, sampling_loss\n",
    "    \n",
    "def upscaler_critic(small, big, scope):\n",
    "    # small may be None, if this is the first layer:\n",
    "    with tf.variable_scope(scope):\n",
    "        _, h, w, _ = [x.value for x in big.get_shape()]\n",
    "        if small:\n",
    "            small_upscaled = tf.image.resize_images(small, [h, w])\n",
    "            img = tf.concat([big, small_upscaled], axis=3)\n",
    "        n_layers = 2\n",
    "        for i in xrange(n_layers):\n",
    "            n_channels = 32 * 2**i\n",
    "            img = layers.conv2d(img, \n",
    "                                n_channels, \n",
    "                                scope='critic_'+str(i),\n",
    "                                kernel_size=5,\n",
    "                                stride=2,\n",
    "                                activation_fn=lrelu,\n",
    "                                normalizer_fn=layers.batch_norm, \n",
    "                                weights_initializer=initializer)\n",
    "        # once we've downsampled using convolution, a final layer computes critic scores across the image,\n",
    "        # which are averaged:\n",
    "        img = layers.conv2d(img,1,\n",
    "                            scope='final_'+str(i),\n",
    "                            kernel_size=5,\n",
    "                            stride=1,\n",
    "                            activation_fn=identity,\n",
    "                            weights_initializer=initializer)\n",
    "        return tf.reduce_mean(img, axis=[1,2])\n",
    "    \n",
    "scopename = 'pg'\n",
    "with tf.variable_scope(scopename):\n",
    "    attributes = ['Male', 'Smiling']\n",
    "    attr_indices = [keys.index(attr) for attr in attributes]\n",
    "    cond_vec = tf.transpose(tf.gather(tf.transpose(attrs_batch, [1, 0]), attr_indices), [1, 0])\n",
    "    \n",
    "    # generator:\n",
    "    z = tf.random_uniform([BATCH_SIZE, Z_SIZE], minval=-1, maxval=1)\n",
    "    gen_image_layers = [tf.reshape(z, [-1, 1, 1, Z_SIZE])]\n",
    "    gen_losses = []\n",
    "    for i in xrange(PYRAMID_LAYERS):\n",
    "        gen_img, scale_loss = upscaler(gen_image_layers[-1], z, 'upscaler_' + str(i))\n",
    "        gen_image_layers.append(gen_img)\n",
    "        gen_losses.append(scale_loss)\n",
    "    \n",
    "    # critic:\n",
    "    for i in xrange(PYRAMID_LAYERS):\n",
    "        pass\n",
    "    \n",
    "    real_disc_output = discriminator(images_batch, cond_vec)\n",
    "    fake_disc_output = discriminator(reconstruction, cond_vec)\n",
    "#     reals_are_real = tf.reduce_mean(l2_loss(real_disc_output, tf.ones_like(real_disc_output)))\n",
    "#     fakes_are_real = tf.reduce_mean(l2_loss(fake_disc_output, tf.ones_like(fake_disc_output)))\n",
    "#     fakes_are_fake = tf.reduce_mean(l2_loss(fake_disc_output, -tf.ones_like(fake_disc_output)))\n",
    "    # the critic outputs large positive numbers for real images, and larhe negative numbers for fake ones:\n",
    "    reals_are_real = tf.reduce_mean(-real_disc_output)\n",
    "    fakes_are_real = tf.reduce_mean(-fake_disc_output)\n",
    "    fakes_are_fake = tf.reduce_mean(fake_disc_output)\n",
    "    \n",
    "    diff_loss = tf.reduce_mean(l2_loss(images_batch, reconstruction))\n",
    "    kl = kl_divergence(z_mean, z_log_stddev)\n",
    "    gen_loss = diff_loss + kl + (fakes_are_real if USE_GAN else 0)\n",
    "    \n",
    "    disc_loss = reals_are_real + fakes_are_fake\n",
    "    \n",
    "    disc_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scopename+'/discriminator')\n",
    "    gen_vars = [v for v in tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES) if v not in disc_vars]\n",
    "    \n",
    "    global_step = tf.contrib.framework.get_or_create_global_step()\n",
    "    train_gen = tf.train.AdamOptimizer(0.001).minimize(gen_loss, var_list=gen_vars, global_step=global_step)\n",
    "    \n",
    "    train_disc_op = tf.train.RMSPropOptimizer(0.0001).minimize(disc_loss, var_list=disc_vars)\n",
    "    weight_clip_op = tf.group(*[w.assign(tf.clip_by_value(w, -0.01, 0.01)) for w in disc_vars])\n",
    "    train_disc = tf.group(weight_clip_op, train_disc_op)\n",
    "\n",
    "print 'ok'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Will not save progress\n"
     ]
    }
   ],
   "source": [
    "session = None\n",
    "saver = None\n",
    "save_path = None\n",
    "\n",
    "def create_session():\n",
    "    global session\n",
    "    global saver\n",
    "    global save_path\n",
    "    \n",
    "    if session: session.close()\n",
    "    \n",
    "    session = tf.InteractiveSession()\n",
    "\n",
    "    save_path = None # 'models/lsgan-face-cond-1'\n",
    "    \n",
    "    init_op = tf.group(tf.global_variables_initializer(), tf.local_variables_initializer())\n",
    "    session.run(init_op)\n",
    "    tf.train.start_queue_runners(sess=session)\n",
    "\n",
    "    import os\n",
    "    saver = None\n",
    "    if save_path:\n",
    "        if not os.path.exists(save_path):\n",
    "            os.mkdir(save_path)\n",
    "        saver = tf.train.Saver()\n",
    "        ckpt = tf.train.get_checkpoint_state(save_path)\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(session, ckpt.model_checkpoint_path)\n",
    "            print 'Restored from checkpoint', ckpt.model_checkpoint_path\n",
    "        else:\n",
    "            print 'Did not restore from checkpoint'\n",
    "    else:\n",
    "        print 'Will not save progress'\n",
    "\n",
    "create_session()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-f14c6f800b30>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdisc_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisc_loss_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_loss_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_gen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_loss\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeed\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mgen_losses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_loss_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def avg(x): return sum(x) / float(len(x))\n",
    "def flatten(l):\n",
    "    return [item for sublist in l for item in sublist]\n",
    "\n",
    "savecount = 0\n",
    "\n",
    "gen_losses = []\n",
    "disc_losses = []\n",
    "\n",
    "while True:\n",
    "    examples_ = None\n",
    "    step_ = global_step.eval()\n",
    "    \n",
    "    feed = {}\n",
    "    \n",
    "    if USE_GAN:\n",
    "        _, _, gen_loss_, disc_loss_ = session.run([train_gen, train_disc, gen_loss, disc_loss], feed_dict=feed)\n",
    "        gen_losses.append(gen_loss_)\n",
    "        disc_losses.append(disc_loss_)\n",
    "    else:\n",
    "        _, gen_loss_ = session.run([train_gen, gen_loss], feed_dict=feed)\n",
    "        gen_losses.append(gen_loss_)\n",
    "    \n",
    "    if step_ % 200 == 0:\n",
    "        if USE_GAN:\n",
    "            print \"Step: {}, disc loss: {}, gen loss: {}\".format(step_, avg(disc_losses), avg(gen_losses))\n",
    "        else:\n",
    "            print \"Step: {}, gen loss: {}\".format(step_, avg(gen_losses))\n",
    "        disc_losses = []\n",
    "        gen_losses = []\n",
    "        \n",
    "        if step_ % 800 == 0:\n",
    "            examples_, originals_ = session.run([reconstruction[:3], images_batch[:3]])\n",
    "            imshow_multi(flatten(zip(originals_, examples_)))\n",
    "        \n",
    "        if step_ % 2000 == 0 and saver:\n",
    "            should_save = True\n",
    "            if should_save:\n",
    "                saver.save(session, save_path + '/model.ckpt', global_step=step_)\n",
    "                print 'Saved'\n",
    "                savecount += 1\n",
    "                if savecount > 4:\n",
    "                    create_session()\n",
    "                    savecount = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00730133 0.095987\n"
     ]
    }
   ],
   "source": [
    "kl_loss_, diff_loss_ = session.run([kl, diff_loss])\n",
    "print kl_loss_, diff_loss_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
